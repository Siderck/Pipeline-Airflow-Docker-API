# ğŸ“Š ETL con Airflow para consumir datos desde una API
---

## ğŸ“‹ DescripciÃ³n del Proyecto
Este proyecto es una canalizaciÃ³n de datos construida con **Apache Airflow** para recopilar informaciÃ³n sobre productos de la categorÃ­a **"Alimentos y Bebidas"** de MercadoLibre, almacenarlos en una base de datos **PostgreSQL** y calcular descuentos de los productos mÃ¡s relevantes.

---

## ğŸ”§ Arquitectura

![Arquitectura del Proyecto](images/diagram.png)


---

## ğŸŒŸ CaracterÃ­sticas principales
- ğŸ” **ExtracciÃ³n de datos**: Uso de la API pÃºblica de MercadoLibre para acceder a informaciÃ³n de productos.
- ğŸ§¹ **TransformaciÃ³n de datos**:
  - Limpieza de datos y normalizaciÃ³n usando **Python** y **Pandas**.
  - CÃ¡lculo del **porcentaje de descuento** sobre el precio original.
- ğŸ—„ï¸ **Carga en base de datos**:
  - InserciÃ³n de datos procesados en **PostgreSQL**, optimizando su almacenamiento para bÃºsquedas rÃ¡pidas y evitando duplicados.
- ğŸ”„ **AutomatizaciÃ³n diaria**:
  - ImplementaciÃ³n del flujo ETL programado con **Apache Airflow** para ejecutar la canalizaciÃ³n de forma automatizada.
- âœ… **Pruebas unitarias**:
  - ImplementaciÃ³n de **pruebas unitarias** para detectar errores en el proceso ETL, especialmente en la manipulaciÃ³n de datos y la conexiÃ³n a la base de datos.

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

### Backend
- ğŸŒ **API REST**: InteracciÃ³n con la API pÃºblica de MercadoLibre para extraer datos de productos.
- ğŸ **Python**:
  - Manejo de solicitudes HTTP con `requests`.
  - Procesamiento de datos con **Pandas**.
  - Escritura y limpieza de archivos CSV para manejar grandes volÃºmenes de datos.
  - **Pruebas unitarias** para validar el flujo de procesamiento de datos.
- ğŸ˜ **PostgreSQL**:
  - DiseÃ±o de tablas para almacenamiento eficiente.
  - Uso de Ã­ndices para mejorar el rendimiento en consultas.
  - Evitar duplicados con claves primarias y restricciones.

### OrquestaciÃ³n
- â±ï¸ **Apache Airflow**:
  - OrquestaciÃ³n y gestiÃ³n de tareas ETL.
  - DefiniciÃ³n de flujos de trabajo modulares usando `PythonOperator` y `PostgresOperator`.

### Infraestructura
- ğŸ³ **Docker**:
  - ContenedorizaciÃ³n del entorno de desarrollo para garantizar consistencia y portabilidad.
- **PostgresHook**:
  - ConexiÃ³n eficiente entre Airflow y la base de datos PostgreSQL.

---

## ğŸš€ Flujo de trabajo ETL

### ExtracciÃ³n de datos
1. Consumo de la **API de MercadoLibre** para obtener productos de la categorÃ­a **"Alimentos y Bebidas"**.
2. Los datos se guardan en un archivo **CSV** como punto intermedio.

### TransformaciÃ³n de datos
- Limpieza de campos inconsistentes o faltantes.
- CÃ¡lculo del **porcentaje de descuento** para cada producto (si aplica).

### Carga de datos
1. InserciÃ³n de los datos en una tabla temporal (**staging**).
2. Transferencia a la tabla principal con eliminaciÃ³n de duplicados.

---

## ğŸ§‘â€ğŸ’» EjecuciÃ³n del proyecto

1. Clona este repositorio:
   ```bash
   git clone https://github.com/tu_usuario/etl-mercadolibre-airflow.git
   cd etl-mercadolibre-airflow

2. Levanta el entorno de Docker:
```bash
docker-compose up -d
```

3. Ejecuta el DAG `create_tables` en **Airflow** para crear las tablas necesarias en **PostgreSQL**.


4. Ejecuta el DAG `etl_to_postgres` desde la interfaz de **Airflow** para iniciar la canalizaciÃ³n.

---

### ğŸ“‚ Estructura del proyecto
```plaintext
.
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ data_pipeline.py       # DefiniciÃ³n del flujo ETL
â”‚   â”œâ”€â”€ create_tables.py       # DefiniciÃ³n de tablas en PostgreSQL
â”œâ”€â”€ docker-compose.yml         # ConfiguraciÃ³n para Docker
â”œâ”€â”€ requirements.txt           # Dependencias del proyecto
â”œâ”€â”€ README.md                  # Este archivo
```

---

### ğŸ¯ Habilidades demostradas

#### ğŸ”µ Desarrollo y automatizaciÃ³n de canalizaciones de datos:
- DiseÃ±o y ejecuciÃ³n de flujos ETL automatizados con **Apache Airflow**.
- OptimizaciÃ³n de procesos para manejar grandes volÃºmenes de datos.

#### ğŸŸ¢ AnÃ¡lisis y limpieza de datos:
- TransformaciÃ³n y enriquecimiento de datos con **Python** y **Pandas**.
- ValidaciÃ³n de datos faltantes e implementaciÃ³n de reglas de negocio.

#### ğŸŸ¡ GestiÃ³n de bases de datos:
- DiseÃ±o de tablas relacionales para optimizar el almacenamiento y las consultas.
- Manejo de duplicados y mejora de Ã­ndices para consultas eficientes.

#### âšª OrquestaciÃ³n y despliegue:
- Uso de **Docker** para la contenedorizaciÃ³n del proyecto, facilitando la portabilidad.
- ConfiguraciÃ³n de conexiones seguras entre servicios.

#### ğŸ”´ InteracciÃ³n con APIs:
- Manejo avanzado de **APIs REST** para extracciÃ³n de datos en tiempo real.

#### ğŸŸ£ Pruebas unitarias:
- ImplementaciÃ³n de pruebas unitarias para validar la correcta ejecuciÃ³n de cada parte del flujo ETL, incluyendo el manejo de excepciones y validaciÃ³n de datos.
